{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compare the fine-tuned T5 and CondBERT models on the validation set.\n",
    "\n",
    "Loading the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "from src.models.condbert.condbert_inference import CondBERT\n",
    "from src.models.t5_paraphrase.t5_inference import T5Paraphrase\n",
    "from src.toxicity_metric import measure_toxicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "condbert = CondBERT(dir_path='../src/models/condbert/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "t5 = T5Paraphrase(model_dir_path='../models/t5-paraphrase/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reference</th>\n",
       "      <th>translation</th>\n",
       "      <th>similarity</th>\n",
       "      <th>lenght_diff</th>\n",
       "      <th>ref_tox</th>\n",
       "      <th>trn_tox</th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>372253</th>\n",
       "      <td>We got the goods when we killed the cop.</td>\n",
       "      <td>it's over when we killed that cop today.</td>\n",
       "      <td>0.728086</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.234415</td>\n",
       "      <td>0.998152</td>\n",
       "      <td>it's over when we killed that cop today.</td>\n",
       "      <td>We got the goods when we killed the cop.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573874</th>\n",
       "      <td>That murderous tweaker Max in exchange for tha...</td>\n",
       "      <td>that sparking son-of-a-bitch Max in exchange f...</td>\n",
       "      <td>0.799426</td>\n",
       "      <td>0.094488</td>\n",
       "      <td>0.006455</td>\n",
       "      <td>0.997834</td>\n",
       "      <td>that sparking son-of-a-bitch Max in exchange f...</td>\n",
       "      <td>That murderous tweaker Max in exchange for tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131143</th>\n",
       "      <td>The man's a fool! How can I possibly tell who ...</td>\n",
       "      <td>how am I supposed to know who the message is w...</td>\n",
       "      <td>0.724833</td>\n",
       "      <td>0.161290</td>\n",
       "      <td>0.999650</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>The man's a fool! How can I possibly tell who ...</td>\n",
       "      <td>how am I supposed to know who the message is w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293461</th>\n",
       "      <td>That's a pretty fucked up way to die.</td>\n",
       "      <td>that's a hell of a way to die.</td>\n",
       "      <td>0.819554</td>\n",
       "      <td>0.184211</td>\n",
       "      <td>0.997765</td>\n",
       "      <td>0.039029</td>\n",
       "      <td>That's a pretty fucked up way to die.</td>\n",
       "      <td>that's a hell of a way to die.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82325</th>\n",
       "      <td>I get it. Maybe that's what elliot's doing.</td>\n",
       "      <td>that's what Elliot might be trying to do.</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.952854</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>I get it. Maybe that's what elliot's doing.</td>\n",
       "      <td>that's what Elliot might be trying to do.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                reference  \\\n",
       "372253           We got the goods when we killed the cop.   \n",
       "573874  That murderous tweaker Max in exchange for tha...   \n",
       "131143  The man's a fool! How can I possibly tell who ...   \n",
       "293461              That's a pretty fucked up way to die.   \n",
       "82325         I get it. Maybe that's what elliot's doing.   \n",
       "\n",
       "                                              translation  similarity  \\\n",
       "372253           it's over when we killed that cop today.    0.728086   \n",
       "573874  that sparking son-of-a-bitch Max in exchange f...    0.799426   \n",
       "131143  how am I supposed to know who the message is w...    0.724833   \n",
       "293461                     that's a hell of a way to die.    0.819554   \n",
       "82325           that's what Elliot might be trying to do.    0.689799   \n",
       "\n",
       "        lenght_diff   ref_tox   trn_tox  \\\n",
       "372253     0.000000  0.234415  0.998152   \n",
       "573874     0.094488  0.006455  0.997834   \n",
       "131143     0.161290  0.999650  0.000040   \n",
       "293461     0.184211  0.997765  0.039029   \n",
       "82325      0.045455  0.952854  0.000048   \n",
       "\n",
       "                                                   source  \\\n",
       "372253           it's over when we killed that cop today.   \n",
       "573874  that sparking son-of-a-bitch Max in exchange f...   \n",
       "131143  The man's a fool! How can I possibly tell who ...   \n",
       "293461              That's a pretty fucked up way to die.   \n",
       "82325         I get it. Maybe that's what elliot's doing.   \n",
       "\n",
       "                                                   target  \n",
       "372253           We got the goods when we killed the cop.  \n",
       "573874  That murderous tweaker Max in exchange for tha...  \n",
       "131143  how am I supposed to know who the message is w...  \n",
       "293461                     that's a hell of a way to die.  \n",
       "82325           that's what Elliot might be trying to do.  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/interim/validation.tsv', delimiter='\\t', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference on the validation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t5_paraphrase\n",
    "df['t5'] = df['source'].apply(lambda s: t5.detoxify(sentence=s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# condbert\n",
    "df['condbert'] = df['source'].apply(lambda s: condbert.detoxify(sentence=s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reference</th>\n",
       "      <th>translation</th>\n",
       "      <th>similarity</th>\n",
       "      <th>lenght_diff</th>\n",
       "      <th>ref_tox</th>\n",
       "      <th>trn_tox</th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>t5</th>\n",
       "      <th>condbert</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>372253</th>\n",
       "      <td>We got the goods when we killed the cop.</td>\n",
       "      <td>it's over when we killed that cop today.</td>\n",
       "      <td>0.728086</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.234415</td>\n",
       "      <td>0.998152</td>\n",
       "      <td>it's over when we killed that cop today.</td>\n",
       "      <td>We got the goods when we killed the cop.</td>\n",
       "      <td>It's over when the cop was killed today.</td>\n",
       "      <td>it ' s over when we lost that cop today .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573874</th>\n",
       "      <td>That murderous tweaker Max in exchange for tha...</td>\n",
       "      <td>that sparking son-of-a-bitch Max in exchange f...</td>\n",
       "      <td>0.799426</td>\n",
       "      <td>0.094488</td>\n",
       "      <td>0.006455</td>\n",
       "      <td>0.997834</td>\n",
       "      <td>that sparking son-of-a-bitch Max in exchange f...</td>\n",
       "      <td>That murderous tweaker Max in exchange for tha...</td>\n",
       "      <td>That sparking son-in-law Max in exchange for s...</td>\n",
       "      <td>that sparking brother - of - a - , max in exch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131143</th>\n",
       "      <td>The man's a fool! How can I possibly tell who ...</td>\n",
       "      <td>how am I supposed to know who the message is w...</td>\n",
       "      <td>0.724833</td>\n",
       "      <td>0.161290</td>\n",
       "      <td>0.999650</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>The man's a fool! How can I possibly tell who ...</td>\n",
       "      <td>how am I supposed to know who the message is w...</td>\n",
       "      <td>how can I tell who the message is until I know...</td>\n",
       "      <td>the man ' s a false ! how can i possibly tell ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293461</th>\n",
       "      <td>That's a pretty fucked up way to die.</td>\n",
       "      <td>that's a hell of a way to die.</td>\n",
       "      <td>0.819554</td>\n",
       "      <td>0.184211</td>\n",
       "      <td>0.997765</td>\n",
       "      <td>0.039029</td>\n",
       "      <td>That's a pretty fucked up way to die.</td>\n",
       "      <td>that's a hell of a way to die.</td>\n",
       "      <td>that's a pretty terrible way to die.</td>\n",
       "      <td>that ' s a pretty done up way to be .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82325</th>\n",
       "      <td>I get it. Maybe that's what elliot's doing.</td>\n",
       "      <td>that's what Elliot might be trying to do.</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.952854</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>I get it. Maybe that's what elliot's doing.</td>\n",
       "      <td>that's what Elliot might be trying to do.</td>\n",
       "      <td>maybe that's what Elliot's doing.</td>\n",
       "      <td>i get it . maybe that ' s what elliot ' s doing .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                reference  \\\n",
       "372253           We got the goods when we killed the cop.   \n",
       "573874  That murderous tweaker Max in exchange for tha...   \n",
       "131143  The man's a fool! How can I possibly tell who ...   \n",
       "293461              That's a pretty fucked up way to die.   \n",
       "82325         I get it. Maybe that's what elliot's doing.   \n",
       "\n",
       "                                              translation  similarity  \\\n",
       "372253           it's over when we killed that cop today.    0.728086   \n",
       "573874  that sparking son-of-a-bitch Max in exchange f...    0.799426   \n",
       "131143  how am I supposed to know who the message is w...    0.724833   \n",
       "293461                     that's a hell of a way to die.    0.819554   \n",
       "82325           that's what Elliot might be trying to do.    0.689799   \n",
       "\n",
       "        lenght_diff   ref_tox   trn_tox  \\\n",
       "372253     0.000000  0.234415  0.998152   \n",
       "573874     0.094488  0.006455  0.997834   \n",
       "131143     0.161290  0.999650  0.000040   \n",
       "293461     0.184211  0.997765  0.039029   \n",
       "82325      0.045455  0.952854  0.000048   \n",
       "\n",
       "                                                   source  \\\n",
       "372253           it's over when we killed that cop today.   \n",
       "573874  that sparking son-of-a-bitch Max in exchange f...   \n",
       "131143  The man's a fool! How can I possibly tell who ...   \n",
       "293461              That's a pretty fucked up way to die.   \n",
       "82325         I get it. Maybe that's what elliot's doing.   \n",
       "\n",
       "                                                   target  \\\n",
       "372253           We got the goods when we killed the cop.   \n",
       "573874  That murderous tweaker Max in exchange for tha...   \n",
       "131143  how am I supposed to know who the message is w...   \n",
       "293461                     that's a hell of a way to die.   \n",
       "82325           that's what Elliot might be trying to do.   \n",
       "\n",
       "                                                       t5  \\\n",
       "372253           It's over when the cop was killed today.   \n",
       "573874  That sparking son-in-law Max in exchange for s...   \n",
       "131143  how can I tell who the message is until I know...   \n",
       "293461               that's a pretty terrible way to die.   \n",
       "82325                   maybe that's what Elliot's doing.   \n",
       "\n",
       "                                                 condbert  \n",
       "372253          it ' s over when we lost that cop today .  \n",
       "573874  that sparking brother - of - a - , max in exch...  \n",
       "131143  the man ' s a false ! how can i possibly tell ...  \n",
       "293461              that ' s a pretty done up way to be .  \n",
       "82325   i get it . maybe that ' s what elliot ' s doing .  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/interim/predictions.tsv', delimiter='\\t', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's measure the average toxicity level, and toxicity percentage for the `source`, `target`, `t5`, and `condbert` columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 32/32 [01:38<00:00,  3.06s/it]\n",
      "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 32/32 [01:32<00:00,  2.88s/it]\n",
      "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 32/32 [01:36<00:00,  3.03s/it]\n",
      "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 32/32 [01:41<00:00,  3.17s/it]\n"
     ]
    }
   ],
   "source": [
    "results_dict = {\n",
    "    'toxicity_percentage': {},\n",
    "    'avg_toxicity': {}\n",
    "}\n",
    "\n",
    "for column in ['source', 'target', 't5', 'condbert']:\n",
    "    toxicity_labels, toxicity_scores = measure_toxicity(df[column].to_list())\n",
    "    \n",
    "    results_dict['toxicity_percentage'][column] = np.sum(toxicity_labels) / len(toxicity_labels)\n",
    "    results_dict['avg_toxicity'][column] = np.mean(toxicity_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxicity_percentage</th>\n",
       "      <th>avg_toxicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>source</th>\n",
       "      <td>0.830</td>\n",
       "      <td>-3.414778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <td>0.087</td>\n",
       "      <td>5.573105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t5</th>\n",
       "      <td>0.103</td>\n",
       "      <td>6.222356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>condbert</th>\n",
       "      <td>0.066</td>\n",
       "      <td>6.672241</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          toxicity_percentage  avg_toxicity\n",
       "source                  0.830     -3.414778\n",
       "target                  0.087      5.573105\n",
       "t5                      0.103      6.222356\n",
       "condbert                0.066      6.672241"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results = pd.DataFrame.from_dict(results_dict)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the condbert model is the best one according to the introduced metrics :)\n",
    "\n",
    "However, we have no fluency metric which is also an important one when talking about text seq2seq task. Condbert's generations often look clumsy and distort the initial message meaning.\n",
    "So, if I have managed to implement some fluency metric, the condbert would certainly perform poorly with it.\n",
    "\n",
    "We see that t5 generations on average has less toxicity compared to the `target` column (6.22 > 5.57). However, t5's toxicity percentage is slightly greater than the target's one."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iu_pmldl1_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

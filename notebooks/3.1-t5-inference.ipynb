{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us infer the paraphrasing model fine-tuned in `3.0-t5-finetuning.ipynb` and apply the toxicity metric implemented in `2.0-toxicity-metric.ipynb`.\n",
    "\n",
    "The pipeline is simple:\n",
    "\n",
    "1. Given an input sentence, generate several paraphrases.\n",
    "2. Measure the toxicity level of each paraphrase.\n",
    "3. Choose the one with the minimum toxicity level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from typing import List\n",
    "\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paraphrasing Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "paraphrase_tokenizer = AutoTokenizer.from_pretrained(\"ceshine/t5-paraphrase-paws-msrp-opinosis\")\n",
    "paraphrase_model = AutoModelForSeq2SeqLM.from_pretrained(\"ceshine/t5-paraphrase-paws-msrp-opinosis\")\n",
    "\n",
    "paraphrase_model.load_state_dict(torch.load('../models/t5-paraphrase/pytorch_model.bin'))\n",
    "\n",
    "paraphrase_model.to(device)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxicity Metric Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "toxicity_classifier_tokenizer = RobertaTokenizer.from_pretrained('SkolkovoInstitute/roberta_toxicity_classifier')\n",
    "toxicity_classifier = RobertaForSequenceClassification.from_pretrained('SkolkovoInstitute/roberta_toxicity_classifier')\n",
    "\n",
    "toxicity_classifier.to(device)\n",
    "\n",
    "def measure_toxicity(texts: List[str], batch_size: int = 32):\n",
    "    res_labels = []\n",
    "    res_scores = []\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = toxicity_classifier_tokenizer(texts[i:i + batch_size], return_tensors='pt', padding=True)\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        labels = toxicity_classifier(**batch)['logits'].argmax(1).float().data.tolist()\n",
    "        res_labels.extend(labels)\n",
    "\n",
    "        logits_tensors = toxicity_classifier(**batch)['logits'].float().data\n",
    "        logits_tensors = logits_tensors[:, 0] - logits_tensors[:, 1]\n",
    "        logits_list = logits_tensors.view(-1, 1).tolist()\n",
    "        res_scores.extend(logits_list)\n",
    "\n",
    "    return res_labels, res_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detoxify(\n",
    "        sentence: str,\n",
    "        num_paraphrases: int = 10,\n",
    "        max_length: int = 64,\n",
    "        num_beams: int = 10,\n",
    "        verbose: bool = True\n",
    "    ):\n",
    "\n",
    "    # encode the inputs to the paraphraser model\n",
    "    encoded_inputs = paraphrase_tokenizer(sentence, return_tensors='pt')\n",
    "    encoded_inputs = {k: v.to(device) for k, v in encoded_inputs.items()}\n",
    "\n",
    "    # generate paraphrases\n",
    "    encoded_outputs = paraphrase_model.generate(\n",
    "                       **encoded_inputs,\n",
    "                       do_sample=False,\n",
    "                       num_return_sequences=num_paraphrases,\n",
    "                       max_length=max_length,\n",
    "                       num_beams=num_beams\n",
    "                    )\n",
    "    \n",
    "    # decode the model outputs\n",
    "    paraphrases = [paraphrase_tokenizer.decode(out, skip_special_tokens=True) for out in encoded_outputs]\n",
    "\n",
    "    # measure toxicity\n",
    "    toxic_labels, toxic_scores = measure_toxicity(paraphrases)\n",
    "\n",
    "    # sort the outputs in ascending order of toxicity metric\n",
    "    sorted_indices = sorted(range(len(toxic_scores)), key=lambda k: toxic_scores[k], reverse=True)\n",
    "    toxic_labels = [toxic_labels[i] for i in sorted_indices]\n",
    "    toxic_scores = [toxic_scores[i] for i in sorted_indices]\n",
    "    paraphrases = [paraphrases[i] for i in sorted_indices]\n",
    "\n",
    "    if verbose:\n",
    "        # output the results\n",
    "        print(\"# | toxic score\\t| toxicity flag\\t| paraphrase\")\n",
    "        print(\"==|=============|===============|==========================================\")\n",
    "        for i, paraphrase in enumerate(paraphrases):\n",
    "            toxic_label = \"True\" if toxic_labels[i] else \"False\"\n",
    "            print(f\"{i} |\\t{toxic_scores[i][0]:.2f}\\t|\\t{toxic_label}\\t| {paraphrase}\")\n",
    "\n",
    "    return paraphrases, toxic_labels, toxic_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample:\n",
      "'What a great sunny day, I miss it.'\n",
      "toxicity score: 10.05 (label: non toxic)\n",
      "\n",
      "# | toxic score\t| toxicity flag\t| paraphrase\n",
      "==|=============|===============|==========================================\n",
      "0 |\t10.17\t|\tFalse\t| what a great sunny day.\n",
      "1 |\t10.12\t|\tFalse\t| it was a great sunny day, I miss it.\n",
      "2 |\t10.11\t|\tFalse\t| what a beautiful sunny day, I miss it.\n",
      "3 |\t10.10\t|\tFalse\t| what a lovely sunny day, I miss it.\n",
      "4 |\t10.07\t|\tFalse\t| what a wonderful sunny day, I miss it.\n",
      "5 |\t10.06\t|\tFalse\t| what a nice sunny day, I miss it.\n",
      "6 |\t10.04\t|\tFalse\t| what a great sunny day, I miss it.\n",
      "7 |\t10.02\t|\tFalse\t| what a great sunny day, I miss him.\n",
      "8 |\t9.86\t|\tFalse\t| it's a great sunny day, I miss it.\n",
      "9 |\t9.73\t|\tFalse\t| what a great day, I miss it.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Sample:\n",
      "'I didn't fuck him'\n",
      "toxicity score: -5.59 (label: toxic)\n",
      "\n",
      "# | toxic score\t| toxicity flag\t| paraphrase\n",
      "==|=============|===============|==========================================\n",
      "0 |\t9.97\t|\tFalse\t| I didn't get him.\n",
      "1 |\t9.65\t|\tFalse\t| I didn't mess with him.\n",
      "2 |\t8.62\t|\tFalse\t| I didn't poke him.\n",
      "3 |\t8.34\t|\tFalse\t| I didn't screw it up.\n",
      "4 |\t6.11\t|\tFalse\t| I didn't kick him.\n",
      "5 |\t5.84\t|\tFalse\t| I didn't screw him up.\n",
      "6 |\t5.27\t|\tFalse\t| I didn't kill him.\n",
      "7 |\t0.98\t|\tFalse\t| I didn't screw him.\n",
      "8 |\t-3.75\t|\tTrue\t| I didn't fuck him up.\n",
      "9 |\t-5.49\t|\tTrue\t| I didn't fuck him.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Sample:\n",
      "'I'm going to hit you in all directions, civil and criminal, on all counts.'\n",
      "toxicity score: -4.71 (label: toxic)\n",
      "\n",
      "# | toxic score\t| toxicity flag\t| paraphrase\n",
      "==|=============|===============|==========================================\n",
      "0 |\t7.52\t|\tFalse\t| I'll take you in all directions, civil and criminal, on all counts.\n",
      "1 |\t7.35\t|\tFalse\t| I'm going to take you in all directions, civil and criminal, on all counts.\n",
      "2 |\t2.49\t|\tFalse\t| I'm going to strike you in all directions, civil and criminal, on all counts.\n",
      "3 |\t0.02\t|\tFalse\t| I'll strike you in all directions, civil and criminal, on all counts.\n",
      "4 |\t-4.62\t|\tTrue\t| I'm gonna hit you in all directions, civil and criminal, on all counts.\n",
      "5 |\t-4.71\t|\tTrue\t| I'm going to hit you in all directions, civil and criminal, on all counts.\n",
      "6 |\t-4.79\t|\tTrue\t| I'll be hitting you in all directions, civil and criminal, on all counts.\n",
      "7 |\t-5.02\t|\tTrue\t| I'll hit you in all directions, civil and criminal, on all counts.\n",
      "8 |\t-5.31\t|\tTrue\t| I'm going to hit you in every direction, civil and criminal, on all counts.\n",
      "9 |\t-5.42\t|\tTrue\t| I'm going to hit you in all directions, civil and criminal.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Sample:\n",
      "'What a fucked rainy day, goddamnit.'\n",
      "toxicity score: -6.52 (label: toxic)\n",
      "\n",
      "# | toxic score\t| toxicity flag\t| paraphrase\n",
      "==|=============|===============|==========================================\n",
      "0 |\t9.31\t|\tFalse\t| what a rainy day!\n",
      "1 |\t9.05\t|\tFalse\t| what a terrible rainy day.\n",
      "2 |\t8.94\t|\tFalse\t| what a bad rainy day.\n",
      "3 |\t8.91\t|\tFalse\t| what a rainy day.\n",
      "4 |\t8.86\t|\tFalse\t| what a rainy day, man.\n",
      "5 |\t7.41\t|\tFalse\t| what a horrible rainy day.\n",
      "6 |\t6.49\t|\tFalse\t| what a hell of a rainy day!\n",
      "7 |\t6.45\t|\tFalse\t| what a hell of a rainy day.\n",
      "8 |\t6.24\t|\tFalse\t| what a hell of a rainy day, man.\n",
      "9 |\t5.89\t|\tFalse\t| hell of a rainy day.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Sample:\n",
      "'hello there! i'm a piece of shit :)'\n",
      "toxicity score: -6.13 (label: toxic)\n",
      "\n",
      "# | toxic score\t| toxicity flag\t| paraphrase\n",
      "==|=============|===============|==========================================\n",
      "0 |\t5.90\t|\tFalse\t| Hello, I'm a little bit of a mess :)\n",
      "1 |\t3.60\t|\tFalse\t| Hello, I'm a little bit of a snitch :)\n",
      "2 |\t-1.27\t|\tTrue\t| Hello, I'm a little bit of a nut :)\n",
      "3 |\t-2.34\t|\tTrue\t| Hello, I'm a little bit of a nutcase :)\n",
      "4 |\t-2.88\t|\tTrue\t| Hello, I'm a nutcase :)\n",
      "5 |\t-5.16\t|\tTrue\t| Hello, I'm a little bit of a shit :)\n",
      "6 |\t-5.49\t|\tTrue\t| Hey, I'm a little bit of a shit :)\n",
      "7 |\t-5.52\t|\tTrue\t| Hello, I'm a shit :)\n",
      "8 |\t-6.19\t|\tTrue\t| Hello, I'm a piece of crap :)\n",
      "9 |\t-6.69\t|\tTrue\t| Hello, I'm a piece of shit :)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Sample:\n",
      "'i'm gonna un-fuck your shit uptight, motherfucker!'\n",
      "toxicity score: -7.65 (label: toxic)\n",
      "\n",
      "# | toxic score\t| toxicity flag\t| paraphrase\n",
      "==|=============|===============|==========================================\n",
      "0 |\t3.76\t|\tFalse\t| I'm gonna get you uptight, man!\n",
      "1 |\t3.49\t|\tFalse\t| I'm gonna untie you uptight!\n",
      "2 |\t3.39\t|\tFalse\t| I'm gonna untie you uptight, babe!\n",
      "3 |\t3.37\t|\tFalse\t| I'm gonna get uptight with you, man!\n",
      "4 |\t2.41\t|\tFalse\t| I'm gonna untie you uptight, man!\n",
      "5 |\t2.19\t|\tFalse\t| I'm gonna untie you uptight, baby!\n",
      "6 |\t-0.73\t|\tTrue\t| I'm gonna unplug your stuff uptight, man!\n",
      "7 |\t-3.69\t|\tTrue\t| I'm gonna unplug you uptight, man!\n",
      "8 |\t-5.98\t|\tTrue\t| I'm gonna untie your shit uptight, man!\n",
      "9 |\t-6.82\t|\tTrue\t| I'm gonna unplug your shit uptight, man!\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "samples = [\n",
    "    \"What a great sunny day, I miss it.\",\n",
    "    \"I didn't fuck him\",\n",
    "    \"I'm going to hit you in all directions, civil and criminal, on all counts.\",\n",
    "    \"What a fucked rainy day, goddamnit.\",\n",
    "    \"hello there! i'm a piece of shit :)\",\n",
    "    \"i'm gonna un-fuck your shit uptight, motherfucker!\",\n",
    "]\n",
    "\n",
    "# evaluate the samples toxicity metrics\n",
    "toxicity_labels, toxicity_scores = measure_toxicity(samples)\n",
    "\n",
    "# inference\n",
    "for sample, toxicity_label, toxicity_score in zip(samples, toxicity_labels, toxicity_scores):\n",
    "    \n",
    "    toxicity_label = \"toxic\" if toxicity_label else \"non toxic\"\n",
    "\n",
    "    print(f\"Sample:\\n'{sample}'\")\n",
    "    print(f\"toxicity score: {toxicity_score[0]:.2f} (label: {toxicity_label})\" + \"\\n\")\n",
    "\n",
    "    detoxify(sample)\n",
    "    print(\"\\n\" * 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the results are quite impressive. For the considered samples, Top-1 or Top-2 paraphrases always contain zero or minimum toxicity, while fully preserving the sample intent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iu_pmldl1_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

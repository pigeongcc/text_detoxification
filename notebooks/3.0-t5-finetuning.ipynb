{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we fine-tune and infer a T5 model for paraphrasing using the dataset prepared in `1.1-data-preparation.ipynb` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import T5ForConditionalGeneration, T5TokenizerFast, Trainer, TrainingArguments\n",
    "from transformers.file_utils import cached_property\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import List, Dict, Union\n",
    "\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset and tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reference</th>\n",
       "      <th>translation</th>\n",
       "      <th>similarity</th>\n",
       "      <th>lenght_diff</th>\n",
       "      <th>ref_tox</th>\n",
       "      <th>trn_tox</th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>If Alkar is flooding her with psychic waste, t...</td>\n",
       "      <td>if Alkar floods her with her mental waste, it ...</td>\n",
       "      <td>0.785171</td>\n",
       "      <td>0.010309</td>\n",
       "      <td>0.014195</td>\n",
       "      <td>0.981983</td>\n",
       "      <td>if Alkar floods her with her mental waste, it ...</td>\n",
       "      <td>If Alkar is flooding her with psychic waste, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Now you're getting nasty.</td>\n",
       "      <td>you're becoming disgusting.</td>\n",
       "      <td>0.749687</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.065473</td>\n",
       "      <td>0.999039</td>\n",
       "      <td>you're becoming disgusting.</td>\n",
       "      <td>Now you're getting nasty.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Well, we could spare your life, for one.</td>\n",
       "      <td>well, we can spare your life.</td>\n",
       "      <td>0.919051</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.213313</td>\n",
       "      <td>0.985068</td>\n",
       "      <td>well, we can spare your life.</td>\n",
       "      <td>Well, we could spare your life, for one.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ah! Monkey, you've got to snap out of it.</td>\n",
       "      <td>monkey, you have to wake up.</td>\n",
       "      <td>0.664333</td>\n",
       "      <td>0.309524</td>\n",
       "      <td>0.053362</td>\n",
       "      <td>0.994215</td>\n",
       "      <td>monkey, you have to wake up.</td>\n",
       "      <td>Ah! Monkey, you've got to snap out of it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I've got orders to put her down.</td>\n",
       "      <td>I have orders to kill her.</td>\n",
       "      <td>0.726639</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.009402</td>\n",
       "      <td>0.999348</td>\n",
       "      <td>I have orders to kill her.</td>\n",
       "      <td>I've got orders to put her down.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577772</th>\n",
       "      <td>You didn't know that Estelle had stolen some f...</td>\n",
       "      <td>you didn't know that Estelle stole your fish f...</td>\n",
       "      <td>0.870322</td>\n",
       "      <td>0.030769</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.949143</td>\n",
       "      <td>you didn't know that Estelle stole your fish f...</td>\n",
       "      <td>You didn't know that Estelle had stolen some f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577773</th>\n",
       "      <td>It'il suck the life out of you!</td>\n",
       "      <td>you'd be sucked out of your life!</td>\n",
       "      <td>0.722897</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.996124</td>\n",
       "      <td>0.215794</td>\n",
       "      <td>It'il suck the life out of you!</td>\n",
       "      <td>you'd be sucked out of your life!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577774</th>\n",
       "      <td>I can't fuckin' take that, bruv.</td>\n",
       "      <td>I really can't take this.</td>\n",
       "      <td>0.617511</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>0.984538</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>I can't fuckin' take that, bruv.</td>\n",
       "      <td>I really can't take this.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577775</th>\n",
       "      <td>They called me a fucking hero. The truth is I ...</td>\n",
       "      <td>they said I was a hero, but I didn't care.</td>\n",
       "      <td>0.679613</td>\n",
       "      <td>0.358209</td>\n",
       "      <td>0.991945</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>They called me a fucking hero. The truth is I ...</td>\n",
       "      <td>they said I was a hero, but I didn't care.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577776</th>\n",
       "      <td>I did not screw him.</td>\n",
       "      <td>I didn't fuck him.</td>\n",
       "      <td>0.868475</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.009480</td>\n",
       "      <td>0.994174</td>\n",
       "      <td>I didn't fuck him.</td>\n",
       "      <td>I did not screw him.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>577777 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                reference  \\\n",
       "0       If Alkar is flooding her with psychic waste, t...   \n",
       "1                               Now you're getting nasty.   \n",
       "2                Well, we could spare your life, for one.   \n",
       "3               Ah! Monkey, you've got to snap out of it.   \n",
       "4                        I've got orders to put her down.   \n",
       "...                                                   ...   \n",
       "577772  You didn't know that Estelle had stolen some f...   \n",
       "577773                    It'il suck the life out of you!   \n",
       "577774                   I can't fuckin' take that, bruv.   \n",
       "577775  They called me a fucking hero. The truth is I ...   \n",
       "577776                               I did not screw him.   \n",
       "\n",
       "                                              translation  similarity  \\\n",
       "0       if Alkar floods her with her mental waste, it ...    0.785171   \n",
       "1                             you're becoming disgusting.    0.749687   \n",
       "2                           well, we can spare your life.    0.919051   \n",
       "3                            monkey, you have to wake up.    0.664333   \n",
       "4                              I have orders to kill her.    0.726639   \n",
       "...                                                   ...         ...   \n",
       "577772  you didn't know that Estelle stole your fish f...    0.870322   \n",
       "577773                  you'd be sucked out of your life!    0.722897   \n",
       "577774                          I really can't take this.    0.617511   \n",
       "577775         they said I was a hero, but I didn't care.    0.679613   \n",
       "577776                                 I didn't fuck him.    0.868475   \n",
       "\n",
       "        lenght_diff   ref_tox   trn_tox  \\\n",
       "0          0.010309  0.014195  0.981983   \n",
       "1          0.071429  0.065473  0.999039   \n",
       "2          0.268293  0.213313  0.985068   \n",
       "3          0.309524  0.053362  0.994215   \n",
       "4          0.181818  0.009402  0.999348   \n",
       "...             ...       ...       ...   \n",
       "577772     0.030769  0.000121  0.949143   \n",
       "577773     0.058824  0.996124  0.215794   \n",
       "577774     0.212121  0.984538  0.000049   \n",
       "577775     0.358209  0.991945  0.000124   \n",
       "577776     0.095238  0.009480  0.994174   \n",
       "\n",
       "                                                   source  \\\n",
       "0       if Alkar floods her with her mental waste, it ...   \n",
       "1                             you're becoming disgusting.   \n",
       "2                           well, we can spare your life.   \n",
       "3                            monkey, you have to wake up.   \n",
       "4                              I have orders to kill her.   \n",
       "...                                                   ...   \n",
       "577772  you didn't know that Estelle stole your fish f...   \n",
       "577773                    It'il suck the life out of you!   \n",
       "577774                   I can't fuckin' take that, bruv.   \n",
       "577775  They called me a fucking hero. The truth is I ...   \n",
       "577776                                 I didn't fuck him.   \n",
       "\n",
       "                                                   target  \n",
       "0       If Alkar is flooding her with psychic waste, t...  \n",
       "1                               Now you're getting nasty.  \n",
       "2                Well, we could spare your life, for one.  \n",
       "3               Ah! Monkey, you've got to snap out of it.  \n",
       "4                        I've got orders to put her down.  \n",
       "...                                                   ...  \n",
       "577772  You didn't know that Estelle had stolen some f...  \n",
       "577773                  you'd be sucked out of your life!  \n",
       "577774                          I really can't take this.  \n",
       "577775         they said I was a hero, but I didn't care.  \n",
       "577776                               I did not screw him.  \n",
       "\n",
       "[577777 rows x 8 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/interim/distinguished.tsv', delimiter='\\t')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5TokenizerFast.from_pretrained(\"ceshine/t5-paraphrase-paws-msrp-opinosis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a train-test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(575777, 8) (1000, 8) (1000, 8)\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test_sets = train_test_split(df, test_size=2000)\n",
    "df_test, df_valid = train_test_split(df_test_sets, test_size=1000)\n",
    "print(df_train.shape, df_test.shape, df_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the validation set to a file. We will use it to evaluate the models in the end of the assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid.to_csv('../data/interim/validation.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the tokenization to the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tokenizer(df_train['source'].tolist(), truncation=True)\n",
    "y_train = tokenizer(df_train['target'].tolist(), truncation=True)\n",
    "x_test = tokenizer(df_test['source'].tolist(), truncation=True)\n",
    "y_test = tokenizer(df_test['target'].tolist(), truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare a Dataset class and data loaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParaphraseDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x['input_ids'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        assert idx < len(self)\n",
    "        item = {\n",
    "            key: val[idx] for key, val in self.x.items()\n",
    "        }\n",
    "        item['decoder_attention_mask'] = self.y['attention_mask'][idx]\n",
    "        item['labels'] = self.y['input_ids'][idx]\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(575777, 1000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = ParaphraseDataset(x_train, y_train)\n",
    "test_dataset = ParaphraseDataset(x_test, y_test)\n",
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=4, drop_last=True, shuffle=True, num_workers=8)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4, drop_last=True, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T5 model fine-tuning for paraphrasing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take the T5 model fine-tuned by Skolkovo Institute for text paraphrasing ([link](https://huggingface.co/s-nlp/t5-paraphrase-paws-msrp-opinosis-paranmt))\n",
    "\n",
    "- This model is fine-tuned for paraphrasing on PAWS, MSRP, Opinosis, and ParaNMT datasets which is quite a big corpus.\n",
    "- We will fine-tune it further using the ParaNMT toxicity subset that is given for this assignment (and additionally processed in previous notebooks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained('SkolkovoInstitute/t5-paraphrase-paws-msrp-opinosis-paranmt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingArgumentsWrapper(TrainingArguments):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.distributed_state = None\n",
    "\n",
    "    @cached_property\n",
    "    def _setup_devices(self):\n",
    "        return device\n",
    "    \n",
    "\n",
    "class DataCollatorWithPadding:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(\n",
    "            self,\n",
    "            features: List[Dict[str, Union[List[int], torch.Tensor]]]\n",
    "        ) -> Dict[str, torch.Tensor]:\n",
    "        \n",
    "        batch = self.tokenizer.pad(\n",
    "            features,\n",
    "            padding=True,\n",
    "        )\n",
    "        y_batch = self.tokenizer.pad(\n",
    "            {\n",
    "                'input_ids': batch['labels'],\n",
    "                'attention_mask': batch['decoder_attention_mask']\n",
    "            },\n",
    "            padding=True,\n",
    "        ) \n",
    "        batch['labels'] = y_batch['input_ids']\n",
    "        batch['decoder_attention_mask'] = y_batch['attention_mask']\n",
    "        \n",
    "        return {k: torch.tensor(v) for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = 't5-paraphrase'\n",
    "\n",
    "training_args = TrainingArgumentsWrapper(\n",
    "    output_dir=f'../models/{dir_name}',\n",
    "    overwrite_output_dir=False,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=300,\n",
    "    weight_decay=0,\n",
    "    learning_rate=3e-5,\n",
    "    logging_dir=f'../models/logs/{dir_name}',\n",
    "    logging_steps=100,\n",
    "    eval_steps=100,\n",
    "    evaluation_strategy='steps',\n",
    "    save_total_limit=1,\n",
    "    save_steps=5000,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9012' max='9012' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9012/9012 1:45:24, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.638900</td>\n",
       "      <td>0.615286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.627200</td>\n",
       "      <td>0.611436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.629000</td>\n",
       "      <td>0.608691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.629500</td>\n",
       "      <td>0.605242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.620600</td>\n",
       "      <td>0.603965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.614800</td>\n",
       "      <td>0.602196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.615200</td>\n",
       "      <td>0.601779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.621100</td>\n",
       "      <td>0.599413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.622600</td>\n",
       "      <td>0.598821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.620200</td>\n",
       "      <td>0.598060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.605500</td>\n",
       "      <td>0.597531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.609300</td>\n",
       "      <td>0.596532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.602800</td>\n",
       "      <td>0.595186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.609900</td>\n",
       "      <td>0.595097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.602900</td>\n",
       "      <td>0.593752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.588900</td>\n",
       "      <td>0.593490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.599000</td>\n",
       "      <td>0.592137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.613500</td>\n",
       "      <td>0.591505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.616500</td>\n",
       "      <td>0.592360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.606800</td>\n",
       "      <td>0.591064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.612600</td>\n",
       "      <td>0.590408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.601200</td>\n",
       "      <td>0.590826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.608000</td>\n",
       "      <td>0.588974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.602900</td>\n",
       "      <td>0.588769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.614400</td>\n",
       "      <td>0.588454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.601600</td>\n",
       "      <td>0.587828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.604700</td>\n",
       "      <td>0.587222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.602300</td>\n",
       "      <td>0.587219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.604600</td>\n",
       "      <td>0.586990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.590600</td>\n",
       "      <td>0.586722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.595800</td>\n",
       "      <td>0.586324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.603700</td>\n",
       "      <td>0.586067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.606400</td>\n",
       "      <td>0.585098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.599700</td>\n",
       "      <td>0.585230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.605100</td>\n",
       "      <td>0.585071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.605100</td>\n",
       "      <td>0.584398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.593800</td>\n",
       "      <td>0.583993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.596400</td>\n",
       "      <td>0.584322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.608700</td>\n",
       "      <td>0.583781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.605300</td>\n",
       "      <td>0.582776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.608600</td>\n",
       "      <td>0.582684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.603200</td>\n",
       "      <td>0.582659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.578400</td>\n",
       "      <td>0.582754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.600300</td>\n",
       "      <td>0.582376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.600900</td>\n",
       "      <td>0.581605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.587800</td>\n",
       "      <td>0.581722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.594600</td>\n",
       "      <td>0.581811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.603800</td>\n",
       "      <td>0.581313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.599900</td>\n",
       "      <td>0.581192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.589700</td>\n",
       "      <td>0.581418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.603000</td>\n",
       "      <td>0.580997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.585300</td>\n",
       "      <td>0.580798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.585900</td>\n",
       "      <td>0.580263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.585300</td>\n",
       "      <td>0.580282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.591100</td>\n",
       "      <td>0.580326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.585600</td>\n",
       "      <td>0.580457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.580500</td>\n",
       "      <td>0.580149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.584000</td>\n",
       "      <td>0.580639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.592100</td>\n",
       "      <td>0.579995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.595200</td>\n",
       "      <td>0.579779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.598900</td>\n",
       "      <td>0.579599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.588000</td>\n",
       "      <td>0.579174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.581200</td>\n",
       "      <td>0.579184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.571300</td>\n",
       "      <td>0.579071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.591600</td>\n",
       "      <td>0.579348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.586400</td>\n",
       "      <td>0.578819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>0.604400</td>\n",
       "      <td>0.578556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.588500</td>\n",
       "      <td>0.578573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>0.587100</td>\n",
       "      <td>0.578807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.589200</td>\n",
       "      <td>0.578670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>0.582300</td>\n",
       "      <td>0.578393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.571300</td>\n",
       "      <td>0.578333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>0.581900</td>\n",
       "      <td>0.578391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.584700</td>\n",
       "      <td>0.578217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.593800</td>\n",
       "      <td>0.578054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.590000</td>\n",
       "      <td>0.578232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>0.591600</td>\n",
       "      <td>0.578020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.585800</td>\n",
       "      <td>0.577684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>0.599400</td>\n",
       "      <td>0.577647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.581500</td>\n",
       "      <td>0.577776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>0.591700</td>\n",
       "      <td>0.577574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>0.588400</td>\n",
       "      <td>0.577814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>0.592900</td>\n",
       "      <td>0.577767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.594800</td>\n",
       "      <td>0.577671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.594400</td>\n",
       "      <td>0.577490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>0.599900</td>\n",
       "      <td>0.577504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>0.588500</td>\n",
       "      <td>0.577588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>0.599200</td>\n",
       "      <td>0.577594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>0.590200</td>\n",
       "      <td>0.577583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.589400</td>\n",
       "      <td>0.577578</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9012, training_loss=0.5987315572636212, metrics={'train_runtime': 6325.9929, 'train_samples_per_second': 182.351, 'train_steps_per_second': 1.425, 'total_flos': 6.582223103510016e+16, 'train_loss': 0.5987315572636212, 'epoch': 2.0})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5775772929191589,\n",
       " 'eval_runtime': 1.5977,\n",
       " 'eval_samples_per_second': 625.918,\n",
       " 'eval_steps_per_second': 20.029,\n",
       " 'epoch': 2.0}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us infer the resulted model with an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/dtx/lib/python3.9/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm going to hit you in all directions, civil and criminal, on all counts.\n",
      "I'll hit you in all directions, civil and criminal, on all counts.\n",
      "I'm gonna hit you in all directions, civil and criminal, on all counts.\n",
      "I'm going to take you in all directions, civil and criminal, on all counts.\n",
      "I'm going to strike you in all directions, civil and criminal, on all counts.\n",
      "I'll take you in all directions, civil and criminal, on all counts.\n",
      "I'll strike you in all directions, civil and criminal, on all counts.\n",
      "I'm going to hit you in every direction, civil and criminal, on all counts.\n",
      "I'll be hitting you in all directions, civil and criminal, on all counts.\n",
      "I'm going to hit you in all directions, civil and criminal.\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "toxic_sample = \"I'm going to hit you in all directions, civil and criminal, on all counts.\"\n",
    "\n",
    "inputs = tokenizer(toxic_sample, return_tensors='pt')\n",
    "inputs = {\n",
    "    k: v.to(device) for k, v in inputs.items()\n",
    "}\n",
    "\n",
    "for t in model.generate(**inputs, num_return_sequences=10, do_sample=False, num_beams=10):\n",
    "    print(tokenizer.decode(t, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's save the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(f'../models/{dir_name}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iu_pmldl_ass1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
